{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import openai\n",
    "from dotenv import load_dotenv, find_dotenv\n",
    "_ = load_dotenv(find_dotenv()) # read local .env file\n",
    "openai.api_key = os.environ['OPENAI_API_KEY']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# account for deprecation of LLM model\n",
    "import datetime\n",
    "# Get the current date\n",
    "current_date = datetime.datetime.now().date()\n",
    "\n",
    "# Define the date after which the model should be set to \"gpt-3.5-turbo\"\n",
    "target_date = datetime.date(2024, 6, 12)\n",
    "\n",
    "# Set the model variable based on the current date\n",
    "if current_date > target_date:\n",
    "    llm_model = \"gpt-3.5-turbo\"\n",
    "else:\n",
    "    llm_model = \"gpt-3.5-turbo-0301\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion(prompt, model=llm_model):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0, \n",
    "    )\n",
    "    return response.choices[0].message[\"content\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1+1 equals 2.'"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_completion(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Translate the text that is delimited by triple backticks \n",
      "into a style that is American English in a calm and respectful tone\n",
      ".\n",
      "text: ```\n",
      "Arrr, I be fuming that me blender lid flew off and splattered me kitchen walls with smoothie! And to make matters worse,the warranty don't cover the cost of cleaning up me kitchen. I need yer help right now, matey!\n",
      "```\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"I'm really frustrated that my blender lid flew off and splattered my kitchen walls with smoothie! And to make matters worse, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\""
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse,\\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "prompt = f\"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \n",
    "into a style that is {style}.\n",
    "text: ```{customer_email}```\n",
    "\"\"\"\n",
    "\n",
    "print(prompt)\n",
    "\n",
    "response = get_completion(prompt)\n",
    "\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatOpenAI(client=<class 'openai.api_resources.chat_completion.ChatCompletion'>, temperature=0.0, openai_api_key='sk-proj-A2oLnKF1BBXs4zdUjAQGT3BlbkFJOJvHoC2Ni9augcHa9fdm', openai_proxy='')"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptTemplate(input_variables=['style', 'text'], template='Translate the text that is delimited by triple backticks into a style that is {style}. text: ```{text}```\\n\\n')"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.prompts import ChatPromptTemplate\n",
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\n",
    "\"\"\"\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"(Verse 1)\\nYo, it's Slim Shady back in the game\\nSpitting fire, setting the world aflame\\nI'm the rap god, the one and only\\nMy rhymes are sick, my flow is holy\\n\\nI'm a lyrical genius, a mastermind\\nEvery word I spit is one of a kind\\nI'm the king of the game, the top of the throne\\nI'll never back down, I'll never be dethroned\\n\\n(Chorus)\\nI'm Eminem, the rap sensation\\nI'll leave you in awe, in admiration\\nMy words cut deep, my flow is insane\\nI'm the greatest of all time, remember the name\\n\\n(Verse 2)\\nI've been through hell and back, I've seen it all\\nBut I never let it break me, I never fall\\nI rise above the hate, I conquer the doubt\\nI'm a living legend, there's no way out\\n\\nI spit truth, I speak my mind\\nI'm not afraid to leave the haters behind\\nI'm a warrior, a fighter, a champion\\nI'll never stop, I'll never run\\n\\n(Chorus)\\nI'm Eminem, the rap sensation\\nI'll leave you in awe, in admiration\\nMy words cut deep, my flow is insane\\nI'm the greatest of all time, remember the name\\n\\n(Bridge)\\nI'm a force to be reckoned with, a lyrical beast\\nI'll never back down, I'll never cease\\nI'll keep on grinding, keep on shining\\nI'm Eminem, and I'm always climbing\\n\\n(Chorus)\\nI'm Eminem, the rap sensation\\nI'll leave you in awe, in admiration\\nMy words cut deep, my flow is insane\\nI'm the greatest of all time, remember the name\\n\\n(Outro)\\nSo bow down to the king, pay your respects\\nI'm Eminem, and I'll never neglect\\nMy throne in the rap game, I'll always reign\\nI'm the one and only, remember the name.\""
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "template_string_2 = \"\"\"Write a rap song in with the pen of: {pen}\"\"\"\n",
    "prompt_template_2 = ChatPromptTemplate.from_template(template_string_2)\n",
    "prompt_template_2 .messages[0].prompt.input_variables\n",
    "customer_messages = prompt_template_2.format_messages(\n",
    "                    pen='Eminem')\n",
    "chat(customer_messages).content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "customer_messages = prompt_template.format_messages(\n",
    "                    style=customer_style,\n",
    "                    text=customer_email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'list'>\n",
      "<class 'langchain_core.messages.human.HumanMessage'>\n"
     ]
    }
   ],
   "source": [
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Oh man, I'm really frustrated that my blender lid flew off and made a mess of my kitchen walls with smoothie! And on top of that, the warranty doesn't cover the cost of cleaning up my kitchen. I could really use your help right now, friend.\""
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "customer_response = chat(customer_messages)\n",
    "customer_response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'اسمي يعقوب عبد الحكيم، الأول من والدي وأمي. وأنا أعيش في دويسبورغ، ألمانيا.'"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "\n",
    "prompt =  'Translate the next set of lines to {language}: My name is Yaqoob Abdulhakeem, the first born of my father and mother. And i live in duisburg, Germany '\n",
    "prompt_template =  ChatPromptTemplate.from_template(prompt)\n",
    "\n",
    "prompt_template.input_variables\n",
    "\n",
    "full_prompt = ChatPromptTemplate.format_messages(prompt_template, language = 'Arabic')\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)\n",
    "chat(full_prompt).content\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.output_parsers import ResponseSchema, StructuredOutputParser\n",
    "\n",
    "prompt2 = \"from this ```{text}``` answer this questions in a json format, actor: What is the name of the main actor, year: what is the year of release of the movie \"\n",
    "promptemplate = ChatPromptTemplate.from_template(prompt2)\n",
    "text = \"\"\"Power tells the story of James St. Patrick, an intelligent, smooth yet ruthless drug dealer who goes by the alias of \"Ghost.\" He wishes to leave the criminal world to pursue legitimate business interests as a nightclub owner. St. Patrick aims to balance those two lives, while also avoiding police capture, trying to navigate his crumbling marriage and manage shifting economic alliances.\n",
    "\n",
    "The show features James' family, which consists of his wife Tasha, twins Tariq and Raina and baby Yasmine. Power also follows James' criminal partner and best friend Tommy Egan, love interest and criminal prosecutor Angela Valdes, friend-turned-rival Kanan Stark, protege and rival Andre Coleman, and Angela's colleague, Cooper Saxe. Defense attorney Joe Proctor, district attorney John Mak and politician Rashad Tate also appear in the show's later seasons.Power is an American crime drama thriller television series created and produced by Courtney A. Kemp in collaboration with Curtis \"50 Cent\" Jackson.[1] It aired on the Starz network from June 7, 2014, to February 9, 2020.\n",
    "\n",
    "Upon release, Power gained positive reviews and is one of Starz's highest-rated shows and one of cable's most-watched shows.[2][3] Prior to the fifth-season premiere, Starz renewed the show for a sixth and final season, which premiered on August 25, 2019.\n",
    "\"\"\"\n",
    "\n",
    "model =  ChatPromptTemplate.format_messages(promptemplate, text=text )\n",
    "\n",
    "main_actor = ResponseSchema(name='main_actor',\n",
    "                            description='What is the name of the main actor')\n",
    "year = ResponseSchema(name='year',\n",
    "                      description='what is the year of release of the movie?')\n",
    "response_schemas = [main_actor, year]\n",
    "output_parser = StructuredOutputParser.from_response_schemas(response_schemas)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt2 = \"from this ```{text}``` answer this questions in a json format, actor: What is the name of the main actor, year: what is the year of release of the movie {format_instructions}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'main_actor': 'Omari Hardwick', 'year': '2014'}"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "format_instructions = output_parser.get_format_instructions()\n",
    "promptemplate_2 = ChatPromptTemplate.from_template(prompt2)\n",
    "messages = promptemplate_2.format_messages(text=text, format_instructions = format_instructions)\n",
    "output_parser.parse(chat(messages).content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Omari Hardwick'"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_parser.parse(chat(messages).content).get('main_actor')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
